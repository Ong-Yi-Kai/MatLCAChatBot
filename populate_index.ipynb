{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SpacyTextSplitter,\\\n",
    "TokenTextSplitter, CharacterTextSplitter\n",
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import streamlit as st\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents:1\n"
     ]
    }
   ],
   "source": [
    "def load_docs(directory:str)-> List:\n",
    "  \"\"\"\n",
    "  Creates a data loader object that generates documents from [directory]\n",
    "  :@param directory: dir to load documents from. Must be a valid dir\n",
    "  \"\"\"\n",
    "  # check for file existance\n",
    "  assert os.path.isdir(directory), f\"{directory} not found.\"\n",
    "\n",
    "  # create a generator object to load documents\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs('./data')\n",
    "print(f\"number of documents:{len(documents)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 219, which is longer than the specified 200\n",
      "Created a chunk of size 243, which is longer than the specified 200\n",
      "Created a chunk of size 205, which is longer than the specified 200\n",
      "Created a chunk of size 562, which is longer than the specified 200\n",
      "Created a chunk of size 326, which is longer than the specified 200\n",
      "Created a chunk of size 295, which is longer than the specified 200\n",
      "Created a chunk of size 312, which is longer than the specified 200\n",
      "Created a chunk of size 313, which is longer than the specified 200\n",
      "Created a chunk of size 215, which is longer than the specified 200\n",
      "Created a chunk of size 202, which is longer than the specified 200\n",
      "Created a chunk of size 206, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT2TokenizerFast\n",
      "Number of chunks: 65\n"
     ]
    }
   ],
   "source": [
    "class UnknownSplitterType(Exception):\n",
    "  pass\n",
    "\n",
    "def split_docs(documents:List,chunk_size:int=500,chunk_overlap:int=100,\n",
    "               splitter_type:str='RecurChar')->List:\n",
    "  \"\"\"\n",
    "  Splits the documents into chunks of [chunk_size] with an overlap of\n",
    "  [chunk_overlap] between adjacent chunks. Splitting regime is base on\n",
    "  [splitter_type].\n",
    "  :@param splitter_type: one of 'RecurChar' (RecursiveCharacterTextSplitter),\n",
    "  'Spacy' (SpacyTextSplitter), 'TikToken' (TokenTextSplitter),\n",
    "  'GPT2TokenizerFast' (uses Hugging Face's GPT2TokenizerFast)\n",
    "  \"\"\"\n",
    "  match splitter_type:\n",
    "    case 'RecurChar':\n",
    "          text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                                         chunk_overlap=chunk_overlap)\n",
    "          print(f\"Loaded RecursiveChatacterTextSplitter\")\n",
    "\n",
    "    case 'Spacy':\n",
    "          text_splitter = SpacyTextSplitter(chunk_size=chunk_size,\n",
    "                                            chunk_overlap=chunk_overlap)\n",
    "          print(f\"Loaded SpacyTextSplitter\")\n",
    "\n",
    "    case 'TikToken':\n",
    "          text_splitter = TokenTextSplitter(chunk_size=chunk_size,\n",
    "                                            chunk_overlap=chunk_overlap)\n",
    "          print(f\"Loaded TikToken\")\n",
    "\n",
    "    case 'GPT2TokenizerFast':\n",
    "          tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "          text_splitter = \\\n",
    "          CharacterTextSplitter.from_huggingface_tokenizer(tokenizer,\n",
    "                                                           chunk_size=chunk_size,\n",
    "                                                           chunk_overlap=chunk_overlap)\n",
    "          print(f\"Loaded GPT2TokenizerFast\")\n",
    "\n",
    "    case _:\n",
    "      raise UnknownSplitterType(\"needs to be one of {'RecurChar', 'Spacy','TikToken','GPT2TokenizerFast'}\")\n",
    "\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "\n",
    "docs = split_docs(documents,chunk_size=200, chunk_overlap=100,splitter_type='GPT2TokenizerFast')\n",
    "print(f\"Number of chunks: {len(docs)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length: 384\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "\n",
    "class UnknownEmbeddingModelType(Exception):\n",
    "  pass\n",
    "\n",
    "def get_embeddings(embedding_model:str, api_key:str=\"\")->str:\n",
    "  \"\"\"\n",
    "  Creates an embedder that would generate the embeddings of the query based on\n",
    "  the specified model\n",
    "  :@param api_key: API key used to query the end point\n",
    "  :@param embedding_model: model name needs to be one of\n",
    "    'OPENAI': Uses OpenAI Embeddings\n",
    "  \"\"\"\n",
    "  match embedding_model:\n",
    "    case 'OPENAI':\n",
    "      assert api_key != \"\", \"OPENAI API key must not be an empty string\"\n",
    "      embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "\n",
    "    case 'sentence_transformers':\n",
    "      embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    case _ :\n",
    "      raise UnknownSplitterType(\"needs to be one of {'OPENAI', sentence_transformers}\")\n",
    "\n",
    "  return embeddings\n",
    "\n",
    "embeddings = get_embeddings('sentence_transformers')\n",
    "query_result = embeddings.embed_query(\"Test\")\n",
    "print(f\"embedding length: {len(query_result)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key= st.secrets.pinecone.api_key,\n",
    "    environment=st.secrets.pinecone.env\n",
    ")\n",
    "\n",
    "index = Pinecone.from_documents(docs, embeddings, index_name=st.secrets.pinecone.index_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
